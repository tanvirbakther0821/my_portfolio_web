<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Comprehensive Learning Guide | Tanvir Bakther</title>
  <meta name="description" content="Complete learning reference covering data processing, machine learning, OpenRefine, Python, Flask, Regular Expressions, and advanced ML algorithms." />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;500;600;700&family=Inter:wght@300;400;500;600&display=swap" rel="stylesheet">
  <style>
    /* ====== REUSED THEME FROM HOMEPAGE ====== */
    *{margin:0;padding:0;box-sizing:border-box}
    :root{--primary:#1a1a1a;--secondary:#f5f5f5;--accent:#d4af37;--white:#fff}
    body{font-family:'Inter',system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial,sans-serif;line-height:1.6;color:var(--secondary);background:#000;overflow-x:hidden}

    /* Navbar */
    .navbar{position:fixed;inset:0 0 auto 0;z-index:1000;padding:20px 50px;background:rgba(0,0,0,.9);backdrop-filter:blur(14px);transition:.3s}
    .navbar.scrolled{background:rgba(0,0,0,.96);padding:14px 50px}
    .nav-container{max-width:1400px;margin:auto;display:flex;align-items:center;justify-content:space-between}
    .logo{font-family:'Playfair Display',serif;font-size:1.6rem;color:#fff;text-decoration:none;letter-spacing:.3px}
    .logo:hover{color:var(--accent)}
    .nav-menu{display:flex;list-style:none;gap:28px}
    .nav-menu a{color:#fff;text-decoration:none;position:relative}
    .nav-menu a::after{content:'';position:absolute;left:0;bottom:-6px;height:2px;width:0;background:var(--accent);transition:.25s}
    .nav-menu a:hover::after,.nav-menu a.active::after{width:100%}

    /* Hero */
    .hero{min-height:60vh;display:grid;place-items:center;position:relative;background:radial-gradient(1200px 800px at 50% 20%, rgba(212,175,55,.08), transparent 60%), #000}
    .hero-bg{position:absolute;inset:0;background:url('https://images.unsplash.com/photo-1517694712202-14dd9538aa97?q=80&w=2400&auto=format&fit=crop') center/cover;filter:brightness(.3) contrast(1.1)}
    .hero-overlay{position:absolute;inset:0;background:linear-gradient(180deg, rgba(0,0,0,.3) 0%, rgba(0,0,0,.8) 100%)}
    .hero-inner{position:relative;z-index:2;max-width:900px;margin-inline:auto;padding:0 18px;text-align:center}
    .kicker{color:var(--accent);letter-spacing:.2em;text-transform:uppercase;font-size:.85rem;margin-bottom:10px}
    .hero-title{font-family:'Playfair Display',serif;font-size:clamp(2.2rem,5vw,3.8rem);font-weight:600;letter-spacing:-.02em;margin:.1em 0 .35em}
    .hero-sub{color:#cfcfcf;font-size:clamp(1rem,2.5vw,1.15rem);max-width:720px;margin:0 auto 26px}

    /* Sections */
    .section{padding:80px 20px;background:#fff;color:#111}
    .section.dark{background:#111;color:#e9e9e9}
    .section.accent{background:#f8f9fa;color:#111}
    .wrap{max-width:1200px;margin:auto}
    .section h2{font-family:'Playfair Display',serif;font-size:clamp(1.8rem,4vw,2.6rem);margin-bottom:16px;color:#d4af37}
    .section h3{font-family:'Playfair Display',serif;font-size:1.6rem;margin:32px 0 16px;color:#d4af37}
    .section h4{font-size:1.2rem;margin:24px 0 12px;color:#333;font-weight:600}
    .dark h4{color:#e9e9e9}
    .section h5{font-size:1rem;margin:16px 0 8px;color:#555;font-weight:600}
    .dark h5{color:#ccc}
    .muted{color:#666}
    .dark .muted{color:#999}

    /* Table of Contents */
    .toc{background:#f8f9fa;border:1px solid #e9ecef;border-radius:8px;padding:24px;margin:24px 0}
    .toc h4{margin-top:0;color:#333}
    .toc ul{list-style:none;padding-left:0}
    .toc li{margin:8px 0}
    .toc a{color:#d4af37;text-decoration:none}
    .toc a:hover{text-decoration:underline}
    .toc-section{margin-bottom:20px}
    .toc-section h5{color:#555;margin-bottom:8px;font-weight:600}

    /* Code blocks */
    .code-section{background:#0a0a0a;border:1px solid #333;border-radius:8px;padding:20px;margin:16px 0;overflow-x:auto}
    .code-section pre{color:#e9e9e9;font-family:'Consolas','Monaco','Courier New',monospace;font-size:.9rem;white-space:pre-wrap;line-height:1.4}
    .code-label{color:var(--accent);font-size:.85rem;margin-bottom:8px;text-transform:uppercase;letter-spacing:.1em;font-weight:600}

    /* Step boxes */
    .step-box{background:#f0f7ff;border-left:4px solid #d4af37;padding:16px;margin:16px 0;border-radius:0 8px 8px 0}
    .step-box h5{color:#333;margin-top:0}
    .dark .step-box{background:#1a1a2e;color:#e9e9e9}

    /* Warning/tip boxes */
    .tip-box{background:rgba(212,175,55,.1);border:1px solid rgba(212,175,55,.3);border-radius:8px;padding:16px;margin:16px 0}
    .tip-box h5{color:var(--accent);margin-top:0}
    .warning-box{background:rgba(220,53,69,.1);border:1px solid rgba(220,53,69,.3);border-radius:8px;padding:16px;margin:16px 0}
    .warning-box h5{color:#dc3545;margin-top:0}

    /* Formula boxes */
    .formula{background:#f8f9fa;border:1px solid #dee2e6;border-radius:8px;padding:16px;margin:16px 0;text-align:center;font-family:'Times New Roman',serif;font-size:1.1em}
    .dark .formula{background:#1a1a2e;border-color:#444;color:#e9e9e9}

    /* File structure display */
    .file-tree{background:#f8f9fa;border:1px solid #e9ecef;padding:16px;border-radius:8px;font-family:monospace;margin:16px 0}
    .file-tree pre{color:#333;margin:0}

    /* Reference tables */
    .ref-table{width:100%;border-collapse:collapse;margin:16px 0;border:1px solid #ddd}
    .ref-table th,.ref-table td{border:1px solid #ddd;padding:12px;text-align:left}
    .ref-table th{background:#f8f9fa;font-weight:600}
    .dark .ref-table{border-color:#444}
    .dark .ref-table th,.dark .ref-table td{border-color:#444}
    .dark .ref-table th{background:#222;color:#e9e9e9}

    /* Future content areas */
    .future-section{border:2px dashed #444;border-radius:8px;padding:24px;margin:32px 0;text-align:center;background:#0a0a0a}
    .future-section h4{color:#666;margin-bottom:8px}
    .future-section p{color:#555;font-style:italic}

    footer{padding:28px 20px;border-top:1px solid #111;background:#000;color:#8c8c8c;text-align:center}

    @media (max-width:900px){
      .navbar{padding:16px 18px}
      .nav-menu{display:none}
    }
  </style>
</head>
<body>
  <!-- ======= NAVBAR ======= -->
  <nav class="navbar" id="navbar">
    <div class="nav-container">
      <a class="logo" href="index.html">Tanvir Bakther</a>
      <ul class="nav-menu">
        <li><a class="nav-link" href="index.html">Home</a></li>
        <li><a class="nav-link" href="photography.html">Photography</a></li>
        <li><a class="nav-link" href="analytics.html">Analytics</a></li>
        <li><a class="nav-link active" href="everyday-learning.html">Everyday Learning</a></li>
        <li><a class="nav-link" href="#contact">Contact</a></li>
      </ul>
    </div>
  </nav>

  <!-- ======= HERO ======= -->
  <header class="hero">
    <div class="hero-bg"></div>
    <div class="hero-overlay"></div>
    <div class="hero-inner">
      <div class="kicker">Comprehensive Learning Reference</div>
      <h1 class="hero-title">Data Science & ML Mastery Guide</h1>
      <p class="hero-sub">Complete tutorials and reference notes for data processing, machine learning algorithms, OpenRefine, Flask development, Regular Expressions, and advanced ML concepts.</p>
    </div>
  </header>

  <!-- ======= TABLE OF CONTENTS ======= -->
  <section class="section">
    <div class="wrap">
      <div class="toc">
        <h4>📚 Complete Learning Reference</h4>
        
        <div class="toc-section">
          <h5>Data Processing Fundamentals</h5>
          <ul>
            <li><a href="#openrefine">1. OpenRefine Data Cleaning Complete Guide</a></li>
            <li><a href="#flask">2. Flask Web Development Tutorial</a></li>
            <li><a href="#regex">3. Regular Expressions Reference</a></li>
            <li><a href="#python-data">4. Python Data Processing Patterns</a></li>
          </ul>
        </div>

        <div class="toc-section">
          <h5>Machine Learning Algorithms</h5>
          <ul>
            <li><a href="#ensemble-methods">5. Ensemble Methods (Boosting vs Bagging)</a></li>
            <li><a href="#cart-decision-trees">6. CART Decision Trees</a></li>
            <li><a href="#compressed-sensing">7. Compressed Sensing & Medical Imaging</a></li>
            <li><a href="#adaboost">8. AdaBoost Algorithm</a></li>
            <li><a href="#random-forest">9. Random Forest</a></li>
            <li><a href="#one-class-svm">10. One-Class SVM</a></li>
            <li><a href="#locally-weighted-regression">11. Locally Weighted Linear Regression</a></li>
          </ul>
        </div>

        <div class="toc-section">
          <h5>Advanced Concepts</h5>
          <ul>
            <li><a href="#bias-variance-tradeoff">12. Bias-Variance Tradeoff</a></li>
            <li><a href="#cross-validation">13. Cross-Validation Techniques</a></li>
            <li><a href="#troubleshooting">14. Common Issues & Solutions</a></li>
            <li><a href="#future">15. Future Learning Areas</a></li>
          </ul>
        </div>
      </div>
    </div>
  </section>

  <!-- ======= 1. OPENREFINE COMPLETE GUIDE ======= -->
  <section class="section dark" id="openrefine">
    <div class="wrap">
      <h2>🔧 OpenRefine Data Cleaning Complete Guide</h2>
      <p class="muted">Master data cleaning, transformation, and quality improvement with OpenRefine</p>

      <h3>Installation & Setup</h3>
      
      <div class="step-box">
        <h5>Step 1: Download OpenRefine</h5>
        <p>Download OpenRefine 3.6.2 from: <code>https://github.com/OpenRefine/OpenRefine/releases/tag/3.6.2</code></p>
        <p>Choose your OS version (Windows, Mac, Linux). Version 3.6.2 includes embedded Java.</p>
      </div>

      <div class="step-box">
        <h5>Step 2: Launch OpenRefine</h5>
        <p>1. Extract the downloaded file</p>
        <p>2. Run the executable (openrefine.exe on Windows)</p>
        <p>3. Open browser and go to: <code>http://127.0.0.1:3333</code></p>
      </div>

      <h3>Project Creation & Data Import</h3>

      <div class="step-box">
        <h5>Creating a New Project</h5>
        <p>1. Click "Create Project"</p>
        <p>2. Choose "This Computer" → Browse for your CSV file</p>
        <p>3. Click "Next" to preview data</p>
        <p>4. Verify column headers and data types</p>
        <p>5. Click "Create Project" (top right)</p>
      </div>

      <div class="tip-box">
        <h5>💡 Data Import Best Practices</h5>
        <p>• Always preview your data before creating the project</p>
        <p>• Check if headers are properly detected</p>
        <p>• Verify column separation (comma, tab, semicolon)</p>
        <p>• Note any encoding issues with special characters</p>
      </div>

      <h3>Core Data Cleaning Operations</h3>

      <h4>1. Removing Blank/Null Values</h4>
      
      <div class="step-box">
        <h5>Using Text Facets to Remove Blanks</h5>
        <p>1. Click on column dropdown → <strong>Facet</strong> → <strong>Text facet</strong></p>
        <p>2. In the facet panel (left side), you'll see all unique values</p>
        <p>3. Click on <strong>(blank)</strong> to select only blank rows</p>
        <p>4. Click <strong>All</strong> → <strong>Edit rows</strong> → <strong>Remove all matching rows</strong></p>
        <p>5. Close the facet when done</p>
      </div>

      <h4>2. Filtering Data</h4>

      <div class="step-box">
        <h5>Text Filtering</h5>
        <p>1. Click column dropdown → <strong>Text filter</strong></p>
        <p>2. Enter your search term (e.g., "Airport")</p>
        <p>3. Note: Filtering is case-sensitive by default</p>
        <p>4. To keep only matching rows: <strong>All</strong> → <strong>Edit rows</strong> → <strong>Remove all non-matching rows</strong></p>
      </div>

      <h4>3. Creating New Columns with GREL</h4>

      <div class="step-box">
        <h5>Adding Columns Based on Existing Data</h5>
        <p>1. Click source column dropdown → <strong>Edit column</strong> → <strong>Add column based on this column</strong></p>
        <p>2. Enter new column name</p>
        <p>3. Write GREL expression in the expression box</p>
        <p>4. Preview shows first few results</p>
        <p>5. Click <strong>OK</strong> to create column</p>
      </div>

      <h3>GREL (General Refine Expression Language) Reference</h3>

      <h4>Essential GREL Functions</h4>

      <table class="ref-table">
        <tr>
          <th>Function</th>
          <th>Purpose</th>
          <th>Example</th>
        </tr>
        <tr>
          <td><code>contains(value, "text")</code></td>
          <td>Check if value contains text</td>
          <td><code>contains(value, "Airport")</code></td>
        </tr>
        <tr>
          <td><code>value.match(/pattern/)</code></td>
          <td>Extract text matching regex</td>
          <td><code>value.match(/([A-Za-z\s]+Airport)/)</code></td>
        </tr>
        <tr>
          <td><code>split(value, delimiter)</code></td>
          <td>Split text into array</td>
          <td><code>split(value, ",")</code></td>
        </tr>
        <tr>
          <td><code>trim(value)</code></td>
          <td>Remove leading/trailing spaces</td>
          <td><code>trim(value)</code></td>
        </tr>
        <tr>
          <td><code>if(condition, true_value, false_value)</code></td>
          <td>Conditional logic</td>
          <td><code>if(contains(value, "Airport"), "Yes", "No")</code></td>
        </tr>
      </table>

      <h4>Real-World GREL Examples</h4>

      <div class="code-section">
        <div class="code-label">Extract Airport Names</div>
        <pre>if(contains(value, "Airport"),
  value.match(/([A-Za-z\s]+Airport)/)[0].trim(),
  ""
)</pre>
      </div>

      <div class="code-section">
        <div class="code-label">Clean and Standardize Text</div>
        <pre>value.trim().replace(/\s+/, " ").toTitlecase()</pre>
      </div>

      <div class="code-section">
        <div class="code-label">Extract Text After Specific Word</div>
        <pre>if(contains(value, " at "),
  split(value, " at ")[1].trim(),
  value
)</pre>
      </div>

      <h3>Clustering & Merging Similar Values</h3>

      <div class="step-box">
        <h5>Using Cluster and Edit Feature</h5>
        <p>1. Select the column with similar values</p>
        <p>2. Go to <strong>Edit cells</strong> → <strong>Cluster and edit</strong></p>
        <p>3. Try different clustering methods:</p>
        <p>   • <strong>Key collision → fingerprint</strong>: Basic similarity</p>
        <p>   • <strong>Key collision → ngram-fingerprint</strong>: Handles typos</p>
        <p>   • <strong>Nearest neighbor → levenshtein</strong>: Character differences</p>
        <p>4. Review suggested clusters and choose merge values</p>
        <p>5. Check <strong>Merge?</strong> for clusters you want to merge</p>
        <p>6. Click <strong>Merge Selected & Re-Cluster</strong></p>
      </div>

      <h3>Exporting Results</h3>

      <div class="step-box">
        <h5>Export Cleaned Data</h5>
        <p>1. Click <strong>Export</strong> (top right) → <strong>Comma-separated value</strong></p>
        <p>2. Save the cleaned CSV file</p>
      </div>

      <div class="step-box">
        <h5>Export History (for Reproducibility)</h5>
        <p>1. Go to <strong>Undo/Redo</strong> tab → <strong>Extract</strong> → <strong>Export</strong></p>
        <p>2. Downloads <code>history.json</code> with all operations</p>
        <p>3. Can be used to replay operations on similar datasets</p>
      </div>

      <div class="warning-box">
        <h5>⚠️ Common Mistakes to Avoid</h5>
        <p>• Always backup original data before major operations</p>
        <p>• Test GREL expressions on small datasets first</p>
        <p>• Check for case sensitivity in filters and matching</p>
        <p>• Verify row counts after filtering operations</p>
      </div>
    </div>
  </section>

  <!-- ======= 2. FLASK WEB DEVELOPMENT TUTORIAL ======= -->
  <section class="section" id="flask">
    <div class="wrap">
      <h2>🌐 Flask Web Development Complete Tutorial</h2>
      <p class="muted">Build data-driven web applications with Python Flask</p>

      <h3>Flask Installation & Setup</h3>

      <div class="step-box">
        <h5>Install Flask</h5>
        <div class="code-section">
          <div class="code-label">Terminal/Command Prompt</div>
          <pre># Install Flask
pip install Flask

# Optional: Create virtual environment first
python -m venv flask_env
# Windows: flask_env\Scripts\activate
# Mac/Linux: source flask_env/bin/activate
pip install Flask</pre>
        </div>
      </div>

      <h3>Basic Flask Application Structure</h3>

      <div class="file-tree">
        <pre>project_folder/
├── run.py                 # Main application runner
├── flaskapp/
│   ├── __init__.py       # Flask app initialization
│   ├── routes.py         # URL routes and view functions
│   └── templates/
│       └── index.html    # HTML templates
└── data/
    └── dataset.csv       # Data files</pre>
      </div>

      <h4>1. Application Runner (run.py)</h4>

      <div class="code-section">
        <div class="code-label">run.py</div>
        <pre>""" run.py - Run the Flask app """
from flaskapp import app

if __name__ == '__main__':
    # Start the Flask development server
    app.run(host='127.0.0.1', port=3001, debug=True)</pre>
      </div>

      <h4>2. Flask App Initialization</h4>

      <div class="code-section">
        <div class="code-label">flaskapp/__init__.py</div>
        <pre>from flask import Flask

# Create Flask application instance
app = Flask(__name__)

# Import routes (must be after app creation)
from flaskapp import routes</pre>
      </div>

      <h4>3. Data Processing Module</h4>

      <div class="code-section">
        <div class="code-label">data_processing.py</div>
        <pre>import csv

def username():
    return 'your_username'

def data_wrangling(filter_class=None):
    """
    Process CSV data and return formatted results
    
    Args:
        filter_class (str): Optional filter for data category
    
    Returns:
        tuple: (header, table_data, dropdown_options)
    """
    with open('data/dataset.csv', 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        table = []
        all_classes = set()
        
        # Read header row
        header = next(reader)
        
        # Read and process data rows
        for row in reader:
            # Convert numeric columns
            processed_row = [
                row[0],                    # species (string)
                row[1],                    # class (string) 
                int(row[2]) if row[2] else 0  # count (integer)
            ]
            table.append(processed_row)
            all_classes.add(row[1])  # Collect unique classes
        
        # Create sorted dropdown options
        dropdown_options = sorted(list(all_classes))
        
        # Apply filtering if specified
        if filter_class:
            table = [row for row in table if row[1] == filter_class]
        
        # Sort by count (descending) and limit to top 10
        table.sort(key=lambda x: x[2], reverse=True)
        table = table[:10]
    
    return header, table, dropdown_options</pre>
      </div>

      <h4>4. Routes and View Functions</h4>

      <div class="code-section">
        <div class="code-label">flaskapp/routes.py</div>
        <pre>from flask import render_template, request
from flaskapp import app
from data_processing import data_wrangling

@app.route('/')
@app.route('/index')
def index():
    """Main page route"""
    # Get filter parameter from URL
    filter_class = request.args.get('filter_class')
    
    # Process data with optional filter
    header, table, dropdown_options = data_wrangling(filter_class)
    
    # Render template with data
    return render_template('index.html',
                         header=header,
                         table=table,
                         dropdown_options=dropdown_options,
                         current_filter=filter_class)

@app.route('/api/data')
def api_data():
    """API endpoint for JSON data"""
    filter_class = request.args.get('filter_class')
    header, table, dropdown_options = data_wrangling(filter_class)
    
    return {
        'header': header,
        'data': table,
        'filters': dropdown_options,
        'count': len(table)
    }</pre>
      </div>

      <h3>Running Your Flask Application</h3>

      <div class="step-box">
        <h5>Start the Development Server</h5>
        <div class="code-section">
          <div class="code-label">Terminal</div>
          <pre># Navigate to your project folder
cd your_project_folder

# Run the application
python run.py

# Output should show:
# * Running on http://127.0.0.1:3001</pre>
        </div>
        <p>Open your browser and go to: <code>http://127.0.0.1:3001</code></p>
      </div>

      <h3>Flask Development Best Practices</h3>

      <div class="tip-box">
        <h5>💡 Essential Tips</h5>
        <p>• Use <code>debug=True</code> for development (auto-reload on changes)</p>
        <p>• Organize code into modules (routes, models, utilities)</p>
        <p>• Use templates for all HTML (avoid HTML in Python code)</p>
        <p>• Handle errors gracefully with try-catch blocks</p>
        <p>• Use environment variables for configuration</p>
      </div>
    </div>
  </section>

  <!-- ======= 3. REGULAR EXPRESSIONS REFERENCE ======= -->
  <section class="section dark" id="regex">
    <div class="wrap">
      <h2>🔍 Regular Expressions Complete Reference</h2>
      <p class="muted">Master pattern matching for text processing and data extraction</p>

      <h3>RegEx Fundamentals</h3>

      <h4>Basic Syntax Elements</h4>
      <table class="ref-table">
        <tr>
          <th>Pattern</th>
          <th>Description</th>
          <th>Example</th>
          <th>Matches</th>
        </tr>
        <tr>
          <td><code>.</code></td>
          <td>Any single character</td>
          <td><code>a.c</code></td>
          <td>abc, axc, a1c</td>
        </tr>
        <tr>
          <td><code>*</code></td>
          <td>Zero or more of preceding</td>
          <td><code>ab*c</code></td>
          <td>ac, abc, abbc</td>
        </tr>
        <tr>
          <td><code>+</code></td>
          <td>One or more of preceding</td>
          <td><code>ab+c</code></td>
          <td>abc, abbc (not ac)</td>
        </tr>
        <tr>
          <td><code>?</code></td>
          <td>Zero or one of preceding</td>
          <td><code>ab?c</code></td>
          <td>ac, abc</td>
        </tr>
        <tr>
          <td><code>^</code></td>
          <td>Start of string</td>
          <td><code>^Hello</code></td>
          <td>Hello world</td>
        </tr>
        <tr>
          <td><code>$</code></td>
          <td>End of string</td>
          <td><code>world$</code></td>
          <td>Hello world</td>
        </tr>
      </table>

      <h4>Data Extraction Patterns</h4>

      <div class="code-section">
        <div class="code-label">Airport Name Extraction</div>
        <pre># Python
import re

# Basic airport pattern
pattern = r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+((?:International\s+)?Airport)\b'

text = "77kg of sandalwood at the Chhatrapati Shivaji Maharaj International Airport"
match = re.search(pattern, text)
if match:
    airport_name = f"{match.group(1)} {match.group(2)}"
    print(airport_name)  # "Chhatrapati Shivaji Maharaj International Airport"</pre>
      </div>

      <div class="code-section">
        <div class="code-label">Email Address Extraction</div>
        <pre># Basic email pattern
pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'

text = "Contact us at support@example.com or admin@site.org"
emails = re.findall(pattern, text)
print(emails)  # ['support@example.com', 'admin@site.org']</pre>
      </div>

      <h3>Python RegEx Functions</h3>

      <table class="ref-table">
        <tr>
          <th>Function</th>
          <th>Purpose</th>
          <th>Returns</th>
        </tr>
        <tr>
          <td><code>re.search(pattern, text)</code></td>
          <td>Find first match</td>
          <td>Match object or None</td>
        </tr>
        <tr>
          <td><code>re.findall(pattern, text)</code></td>
          <td>Find all matches</td>
          <td>List of strings</td>
        </tr>
        <tr>
          <td><code>re.sub(pattern, replacement, text)</code></td>
          <td>Replace matches</td>
          <td>Modified string</td>
        </tr>
      </table>
    </div>
  </section>

  <!-- ======= 4. PYTHON DATA PROCESSING PATTERNS ======= -->
  <section class="section accent" id="python-data">
    <div class="wrap">
      <h2>🐍 Python Data Processing Patterns</h2>
      <p class="muted">Essential Python techniques for data manipulation and analysis</p>

      <h3>Pandas Data Manipulation</h3>

      <h4>Essential Pandas Operations</h4>
      <div class="code-section">
        <div class="code-label">Data Loading and Basic Operations</div>
        <pre>import pandas as pd
import numpy as np

# Load CSV data
df = pd.read_csv('data.csv')

# Basic data exploration
print(f"Shape: {df.shape}")
print(f"Columns: {df.columns.tolist()}")
print(f"Data types:\n{df.dtypes}")
print(f"Missing values:\n{df.isnull().sum()}")

# Display first few rows
print(df.head())

# Basic statistics
print(df.describe())</pre>
      </div>

      <h4>Data Cleaning Patterns</h4>
      <div class="code-section">
        <div class="code-label">Remove Duplicates and Handle Missing Data</div>
        <pre># Remove duplicate rows
df_clean = df.drop_duplicates()

# Remove rows with missing values in specific columns
df_clean = df.dropna(subset=['important_column'])

# Fill missing values
df['column'] = df['column'].fillna('default_value')
df['numeric_column'] = df['numeric_column'].fillna(df['numeric_column'].mean())

# Remove rows where column is empty string
df_clean = df[df['column'].str.strip() != '']</pre>
      </div>

      <h4>Filtering and Querying</h4>
      <div class="code-section">
        <div class="code-label">Advanced Filtering Techniques</div>
        <pre># Basic filtering
airport_data = df[df['Subject'].str.contains('Airport', case=True, na=False)]

# Multiple conditions
filtered = df[
    (df['Count'] > 10) & 
    (df['Country'].isin(['China', 'India'])) &
    (df['Date'].str.contains('2024'))
]

# Using query method (more readable for complex conditions)
result = df.query("Count > 10 and Country in ['China', 'India']")

# String operations
df['clean_name'] = df['name'].str.strip().str.title()
has_keyword = df[df['description'].str.contains('wildlife', case=False, na=False)]</pre>
      </div>
    </div>
  </section>

  <!-- ======= 5. ENSEMBLE METHODS ======= -->
  <section class="section dark" id="ensemble-methods">
    <div class="wrap">
      <h2>🌲 Ensemble Methods (Boosting vs Bagging)</h2>
      <p class="muted">Understanding parallel vs sequential ensemble learning approaches</p>

      <h3>Installation & Setup</h3>
      <div class="code-section">
        <div class="code-label">Required packages</div>
        <pre># Required packages
import numpy as np
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score</pre>
      </div>

      <h3>Core Concepts</h3>
      
      <h4>Bagging (Bootstrap Aggregating)</h4>
      <ul>
        <li><strong>Parallel training:</strong> Multiple models trained independently</li>
        <li><strong>Bootstrap sampling:</strong> Each model sees different subset of data</li>
        <li><strong>Averaging:</strong> Final prediction is average/majority vote</li>
        <li><strong>Variance reduction:</strong> Reduces overfitting by averaging</li>
        <li><strong>Example:</strong> Random Forest</li>
      </ul>

      <h4>Boosting</h4>
      <ul>
        <li><strong>Sequential training:</strong> Models trained one after another</li>
        <li><strong>Error focusing:</strong> Later models focus on previous mistakes</li>
        <li><strong>Weighted combination:</strong> Better models get higher weight</li>
        <li><strong>Bias reduction:</strong> Converts weak learners to strong learner</li>
        <li><strong>Example:</strong> AdaBoost, Gradient Boosting</li>
      </ul>

      <div class="formula">
        <strong>Bagging:</strong> f(x) = (1/M) Σ f_m(x)<br>
        <strong>Boosting:</strong> f(x) = Σ α_m h_m(x)
      </div>

      <h3>Code Examples</h3>
      <div class="code-section">
        <div class="code-label">Implementation Examples</div>
        <pre># Random Forest (Bagging)
rf = RandomForestClassifier(
    n_estimators=100,
    max_features='sqrt',  # Random feature selection
    bootstrap=True,       # Bootstrap sampling
    oob_score=True       # Out-of-bag error estimation
)

# AdaBoost (Boosting)
ada = AdaBoostClassifier(
    base_estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators=50,
    learning_rate=1.0
)</pre>
      </div>

      <h3>Best Practices</h3>
      <div class="tip-box">
        <h5>Random Forest Tips</h5>
        <p>• Use √p features for classification, p/3 for regression</p>
        <p>• More trees rarely hurt (diminishing returns after ~100)</p>
        <p>• Use OOB error for model selection</p>
        <p>• Feature importance provides good interpretability</p>
      </div>

      <div class="tip-box">
        <h5>Boosting Tips</h5>
        <p>• Start with weak learners (shallow trees)</p>
        <p>• Lower learning rate with more estimators</p>
        <p>• Monitor training/validation error to avoid overfitting</p>
        <p>• More sensitive to noise than bagging</p>
      </div>

      <h3>Troubleshooting</h3>
      <div class="warning-box">
        <h5>Common Issues</h5>
        <p>• <strong>Memory issues:</strong> Reduce n_estimators or use n_jobs=1</p>
        <p>• <strong>Slow training:</strong> Use n_jobs=-1 for parallel processing</p>
        <p>• <strong>Overfitting in boosting:</strong> Reduce learning rate or max_depth</p>
        <p>• <strong>Poor Random Forest performance:</strong> Check max_features and min_samples_leaf</p>
      </div>
    </div>
  </section>

  <!-- ======= 6. CART DECISION TREES ======= -->
  <section class="section" id="cart-decision-trees">
    <div class="wrap">
      <h2>🌳 CART Decision Trees</h2>
      <p class="muted">Classification and Regression Trees for interpretable machine learning</p>

      <h3>Installation & Setup</h3>
      <div class="code-section">
        <div class="code-label">Required imports</div>
        <pre>from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree
import matplotlib.pyplot as plt</pre>
      </div>

      <h3>Core Concepts</h3>
      
      <h4>CART Algorithm</h4>
      <ul>
        <li><strong>Binary splits:</strong> Each node has exactly two branches</li>
        <li><strong>Greedy splitting:</strong> Best split at each node independently</li>
        <li><strong>Impurity measures:</strong> Gini, Entropy (classification), MSE (regression)</li>
        <li><strong>Stopping criteria:</strong> Max depth, min samples, impurity threshold</li>
      </ul>

      <div class="formula">
        <strong>Gini Impurity:</strong> G = 1 - Σ p_i²<br>
        <strong>Entropy:</strong> H = -Σ p_i log(p_i)<br>
        <strong>Information Gain:</strong> IG = H(parent) - Σ (n_i/n) H(child_i)
      </div>

      <h4>Overfitting Prevention</h4>
      <ul>
        <li><strong>Pre-pruning:</strong> max_depth, min_samples_split, min_samples_leaf</li>
        <li><strong>Post-pruning:</strong> Cost complexity pruning</li>
        <li><strong>Cross-validation:</strong> For hyperparameter selection</li>
        <li><strong>Early stopping:</strong> Based on validation error</li>
      </ul>

      <h3>Code Examples</h3>
      <div class="code-section">
        <div class="code-label">CART Implementation</div>
        <pre># Classification Tree
clf = DecisionTreeClassifier(
    criterion='gini',        # or 'entropy'
    max_depth=10,           # Limit tree depth
    min_samples_split=20,   # Min samples to split
    min_samples_leaf=10,    # Min samples in leaf
    random_state=42
)

# Visualization
plt.figure(figsize=(20, 10))
plot_tree(clf, 
         feature_names=feature_names,
         class_names=class_names,
         filled=True,
         rounded=True,
         max_depth=3)  # Show only first 3 levels</pre>
      </div>

      <h3>Best Practices</h3>
      <div class="tip-box">
        <h5>Hyperparameter Guidelines</h5>
        <p>• <strong>max_depth:</strong> Start with 5-10, tune based on validation</p>
        <p>• <strong>min_samples_split:</strong> 2-20, higher for noisy data</p>
        <p>• <strong>min_samples_leaf:</strong> 1-10, higher prevents overfitting</p>
        <p>• <strong>max_features:</strong> sqrt(n) for classification, n/3 for regression</p>
      </div>

      <h3>Troubleshooting</h3>
      <div class="warning-box">
        <h5>Common Problems</h5>
        <p>• <strong>Overfitting:</strong> Reduce max_depth, increase min_samples_leaf</p>
        <p>• <strong>Underfitting:</strong> Increase max_depth, reduce stopping criteria</p>
        <p>• <strong>Unbalanced trees:</strong> Check data distribution, consider class weights</p>
        <p>• <strong>Large trees hard to visualize:</strong> Use max_depth=3 in plot_tree</p>
      </div>
    </div>
  </section>

  <!-- ======= 7. COMPRESSED SENSING ======= -->
  <section class="section dark" id="compressed-sensing">
    <div class="wrap">
      <h2>📡 Compressed Sensing & Medical Imaging</h2>
      <p class="muted">Sparse signal recovery from underdetermined systems</p>

      <h3>Installation & Setup</h3>
      <div class="code-section">
        <div class="code-label">Required packages</div>
        <pre>import numpy as np
from sklearn.linear_model import Lasso, Ridge, LassoCV, RidgeCV
import scipy.io
import matplotlib.pyplot as plt</pre>
      </div>

      <h3>Core Concepts</h3>
      
      <h4>Compressed Sensing Theory</h4>
      <ul>
        <li><strong>Sparse signals:</strong> Most coefficients are zero</li>
        <li><strong>Underdetermined systems:</strong> n measurements < p unknowns</li>
        <li><strong>L1 regularization:</strong> Promotes sparsity</li>
        <li><strong>RIP condition:</strong> Restricted Isometry Property for exact recovery</li>
      </ul>

      <div class="formula">
        <strong>Measurement Model:</strong> y = Ax + ε<br>
        <strong>Lasso:</strong> min ||y - Ax||₂² + λ||x||₁<br>
        <strong>Ridge:</strong> min ||y - Ax||₂² + λ||x||₂²
      </div>

      <h4>Lasso vs Ridge for Sparse Recovery</h4>
      <ul>
        <li><strong>Lasso (L1):</strong> Sets coefficients exactly to zero, recovers sparse signals</li>
        <li><strong>Ridge (L2):</strong> Shrinks coefficients, doesn't achieve sparsity</li>
        <li><strong>Medical imaging:</strong> MRI images are naturally sparse in some domain</li>
      </ul>

      <h3>Code Examples</h3>
      <div class="code-section">
        <div class="code-label">Compressed Sensing Implementation</div>
        <pre># Generate sparse signal and measurements
n_measurements, n_pixels = 1300, 2500
A = np.random.normal(0, 1, (n_measurements, n_pixels))
noise = np.random.normal(0, 5, n_measurements)
y = A @ x_true + noise

# Lasso with cross-validation
lasso_cv = LassoCV(alphas=np.logspace(-4, 1, 50), cv=10)
lasso_cv.fit(A, y)
x_lasso = lasso_cv.coef_

# Ridge with cross-validation
ridge_cv = RidgeCV(alphas=np.logspace(-2, 3, 50), cv=10)
ridge_cv.fit(A, y)
x_ridge = ridge_cv.coef_</pre>
      </div>

      <h3>Best Practices</h3>
      <div class="tip-box">
        <h5>Compressed Sensing Guidelines</h5>
        <p>• <strong>Measurement matrix:</strong> Use random Gaussian or Fourier measurements</p>
        <p>• <strong>Lambda selection:</strong> Use cross-validation or information criteria</p>
        <p>• <strong>Sparsity level:</strong> Ensure sufficient measurements (m ≥ 2s log p)</p>
        <p>• <strong>Noise handling:</strong> Account for noise level in regularization</p>
      </div>

      <h3>Troubleshooting</h3>
      <div class="warning-box">
        <h5>Recovery Issues</h5>
        <p>• <strong>Poor recovery:</strong> Check sparsity level and measurement count</p>
        <p>• <strong>Over-regularization:</strong> Lambda too large, reduces to zero</p>
        <p>• <strong>Under-regularization:</strong> Lambda too small, doesn't promote sparsity</p>
        <p>• <strong>Numerical issues:</strong> Scale features, add small regularization</p>
      </div>
    </div>
  </section>

  <!-- ======= 8. ADABOOST ALGORITHM ======= -->
  <section class="section accent" id="adaboost">
    <div class="wrap">
      <h2>⚡ AdaBoost Algorithm</h2>
      <p class="muted">Adaptive boosting for sequential weak learner combination</p>

      <h3>Installation & Setup</h3>
      <div class="code-section">
        <div class="code-label">Required imports</div>
        <pre>import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier</pre>
      </div>

      <h3>Core Concepts</h3>
      
      <h4>AdaBoost Algorithm Steps</h4>
      <ol>
        <li><strong>Initialize weights:</strong> D₁(i) = 1/n for all i</li>
        <li><strong>For t = 1 to T:</strong>
          <ul>
            <li>Train weak learner hₜ with weights Dₜ</li>
            <li>Calculate error: εₜ = Σ Dₜ(i) × I[hₜ(xᵢ) ≠ yᵢ]</li>
            <li>Calculate alpha: αₜ = ½ ln((1-εₜ)/εₜ)</li>
            <li>Update weights: Dₜ₊₁(i) = Dₜ(i) × exp(-αₜyᵢhₜ(xᵢ)) / Zₜ</li>
          </ul>
        </li>
        <li><strong>Final classifier:</strong> H(x) = sign(Σ αₜhₜ(x))</li>
      </ol>

      <div class="formula">
        <strong>Weight Update:</strong> Dₜ₊₁(i) = (Dₜ(i) × exp(-αₜyᵢhₜ(xᵢ))) / Zₜ<br>
        <strong>Alpha:</strong> αₜ = ½ ln((1-εₜ)/εₜ)<br>
        <strong>Normalization:</strong> Zₜ = Σ Dₜ(i) × exp(-αₜyᵢhₜ(xᵢ))
      </div>

      <h3>Code Examples</h3>
      <div class="code-section">
        <div class="code-label">Manual AdaBoost Implementation</div>
        <pre># Manual AdaBoost implementation (key parts)
def find_best_stump(X, y, weights):
    best_error = float('inf')
    best_stump = None
    
    for feature in [0, 1]:  # For each feature
        for threshold in thresholds:
            for polarity in [1, -1]:
                predictions = np.where(X[:, feature] <= threshold, polarity, -polarity)
                error = np.sum(weights[predictions != y])
                
                if error < best_error:
                    best_error = error
                    best_stump = {'feature': feature, 'threshold': threshold, 
                                'polarity': polarity, 'predictions': predictions}
    
    return best_stump, best_error

# Weight updates
alpha = 0.5 * np.log((1 - error) / error)
Z = np.sum(D * np.exp(-alpha * y * predictions))
D_new = D * np.exp(-alpha * y * predictions) / Z</pre>
      </div>

      <h3>Best Practices</h3>
      <div class="tip-box">
        <h5>AdaBoost Guidelines</h5>
        <p>• <strong>Weak learners:</strong> Use shallow trees (stumps or max_depth=1-3)</p>
        <p>• <strong>Number of iterations:</strong> Monitor training error, stop when it plateaus</p>
        <p>• <strong>Data quality:</strong> AdaBoost is sensitive to noise and outliers</p>
        <p>• <strong>Class balance:</strong> Works best with balanced datasets</p>
      </div>

      <h3>Troubleshooting</h3>
      <div class="warning-box">
        <h5>Common Issues</h5>
        <p>• <strong>Overfitting:</strong> Reduce number of estimators or use early stopping</p>
        <p>• <strong>Weak learners too strong:</strong> Reduce tree depth</p>
        <p>• <strong>Numerical instability:</strong> Check for very small errors (ε ≈ 0)</p>
        <p>• <strong>Poor performance:</strong> Ensure weak learners are actually weak but better than random</p>
      </div>
    </div>
  </section>

  <!-- ======= 9. RANDOM FOREST ======= -->
  <section class="section dark" id="random-forest">
    <div class="wrap">
      <h2>🌲 Random Forest</h2>
      <p class="muted">Ensemble of decision trees with bootstrap aggregating</p>

      <h3>Installation & Setup</h3>
      <div class="code-section">
        <div class="code-label">Required packages</div>
        <pre>from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import cross_val_score
import numpy as np</pre>
      </div>

      <h3>Core Concepts</h3>
      
      <h4>Random Forest Components</h4>
      <ul>
        <li><strong>Bootstrap sampling:</strong> Each tree trained on different subset</li>
        <li><strong>Random feature selection:</strong> At each split, consider random subset of features</li>
        <li><strong>Majority voting:</strong> Final prediction by majority vote (classification) or averaging (regression)</li>
        <li><strong>Out-of-bag error:</strong> Unbiased error estimate using unused samples</li>
      </ul>

      <h4>Key Parameters</h4>
      <ul>
        <li><strong>n_estimators:</strong> Number of trees in forest</li>
        <li><strong>max_features:</strong> Number of features to consider at each split</li>
        <li><strong>max_depth:</strong> Maximum depth of trees</li>
        <li><strong>min_samples_split/leaf:</strong> Minimum samples for splitting/leaf nodes</li>
      </ul>

      <div class="formula">
        <strong>OOB Error:</strong> Use ~1/3 of samples not in bootstrap for each tree<br>
        <strong>Feature Importance:</strong> Average decrease in impurity across all trees
      </div>

      <h3>Code Examples</h3>
      <div class="code-section">
        <div class="code-label">Random Forest Implementation</div>
        <pre># Random Forest with parameter tuning
rf = RandomForestClassifier(
    n_estimators=100,
    max_features='sqrt',     # √p for classification
    max_depth=10,
    min_samples_split=10,
    min_samples_leaf=5,
    oob_score=True,         # Out-of-bag error
    random_state=42,
    n_jobs=-1              # Parallel processing
)

# Parameter sensitivity analysis
max_features_range = [1, 2, 5, 10, int(np.sqrt(n_features)), n_features//3]
oob_errors = []
test_errors = []

for max_feat in max_features_range:
    rf_temp = RandomForestClassifier(max_features=max_feat, oob_score=True)
    rf_temp.fit(X_train, y_train)
    oob_errors.append(1 - rf_temp.oob_score_)
    test_errors.append(1 - rf_temp.score(X_test, y_test))</pre>
      </div>

      <h3>Best Practices</h3>
      <div class="tip-box">
        <h5>Optimization Tips</h5>
        <p>• <strong>max_features:</strong> √p for classification, p/3 for regression</p>
        <p>• <strong>n_estimators:</strong> Start with 100, increase until OOB error stabilizes</p>
        <p>• <strong>Tree depth:</strong> Don't limit unless overfitting (Random Forest handles this well)</p>
        <p>• <strong>Feature importance:</strong> Use for feature selection and interpretation</p>
      </div>

      <h3>Troubleshooting</h3>
      <div class="warning-box">
        <h5>Performance Issues</h5>
        <p>• <strong>High bias:</strong> Increase n_estimators, reduce min_samples_leaf</p>
        <p>• <strong>High variance:</strong> Increase min_samples_split, limit max_depth</p>
        <p>• <strong>Slow training:</strong> Use n_jobs=-1, reduce n_estimators</p>
        <p>• <strong>Memory issues:</strong> Reduce max_depth or n_estimators</p>
      </div>
    </div>
  </section>

  <!-- ======= 10. ONE-CLASS SVM ======= -->
  <section class="section" id="one-class-svm">
    <div class="wrap">
      <h2>🎯 One-Class SVM</h2>
      <p class="muted">Novelty detection and anomaly identification</p>

      <h3>Installation & Setup</h3>
      <div class="code-section">
        <div class="code-label">Required imports</div>
        <pre>from sklearn.svm import OneClassSVM
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report</pre>
      </div>

      <h3>Core Concepts</h3>
      
      <h4>Novelty Detection Approach</h4>
      <ul>
        <li><strong>Training:</strong> Learn boundary around "normal" data only</li>
        <li><strong>Prediction:</strong> +1 for normal, -1 for outliers/anomalies</li>
        <li><strong>Use cases:</strong> Fraud detection, spam detection, fault detection</li>
        <li><strong>Assumption:</strong> Anomalies are rare and different from normal data</li>
      </ul>

      <h4>Key Parameters</h4>
      <ul>
        <li><strong>nu (ν):</strong> Fraction of training examples to be outliers (0 < ν ≤ 1)</li>
        <li><strong>gamma:</strong> RBF kernel parameter controlling decision boundary complexity</li>
        <li><strong>kernel:</strong> Usually 'rbf' for non-linear boundaries</li>
      </ul>

      <div class="formula">
        <strong>Decision Function:</strong> f(x) = sign(Σ αᵢ K(xᵢ, x) - ρ)<br>
        <strong>RBF Kernel:</strong> K(x, y) = exp(-γ||x - y||²)
      </div>

      <h3>Code Examples</h3>
      <div class="code-section">
        <div class="code-label">One-Class SVM Implementation</div>
        <pre># One-Class SVM setup
# Extract only normal examples for training
X_normal = X_train[y_train == 0]  # Only non-spam emails

# Feature scaling (important for SVM)
scaler = StandardScaler()
X_normal_scaled = scaler.fit_transform(X_normal)
X_test_scaled = scaler.transform(X_test)

# Hyperparameter tuning
nu_values = [0.01, 0.05, 0.1, 0.2, 0.3, 0.5]
gamma_values = ['scale', 'auto', 0.001, 0.01, 0.1, 1.0]

best_params = {}
best_accuracy = 0

for nu in nu_values:
    for gamma in gamma_values:
        ocsvm = OneClassSVM(nu=nu, gamma=gamma, kernel='rbf')
        ocsvm.fit(X_normal_scaled)
        
        predictions = ocsvm.predict(X_test_scaled)
        # Convert: +1 → 0 (normal), -1 → 1 (anomaly)
        y_pred = np.where(predictions == 1, 0, 1)
        accuracy = accuracy_score(y_test, y_pred)
        
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_params = {'nu': nu, 'gamma': gamma}</pre>
      </div>

      <h3>Best Practices</h3>
      <div class="tip-box">
        <h5>One-Class SVM Guidelines</h5>
        <p>• <strong>Feature scaling:</strong> Always scale features for SVM</p>
        <p>• <strong>Nu parameter:</strong> Start with expected outlier fraction</p>
        <p>• <strong>Gamma tuning:</strong> Use grid search with cross-validation</p>
        <p>• <strong>Evaluation:</strong> Focus on recall for anomaly class</p>
      </div>

      <h3>Troubleshooting</h3>
      <div class="warning-box">
        <h5>Common Challenges</h5>
        <p>• <strong>Poor anomaly detection:</strong> Adjust nu parameter, try different gamma</p>
        <p>• <strong>Too many false positives:</strong> Decrease nu value</p>
        <p>• <strong>Too many false negatives:</strong> Increase nu value</p>
        <p>• <strong>Training instability:</strong> Ensure sufficient normal examples</p>
      </div>
    </div>
  </section>

  <!-- ======= 11. LOCALLY WEIGHTED LINEAR REGRESSION ======= -->
  <section class="section dark" id="locally-weighted-regression">
    <div class="wrap">
      <h2>📈 Locally Weighted Linear Regression</h2>
      <p class="muted">Non-parametric regression with local model fitting</p>

      <h3>Installation & Setup</h3>
      <div class="code-section">
        <div class="code-label">Required packages</div>
        <pre>import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold</pre>
      </div>

      <h3>Core Concepts</h3>
      
      <h4>Mathematical Foundation</h4>
      <ul>
        <li><strong>Local fitting:</strong> Fit linear model locally around prediction point</li>
        <li><strong>Weighted regression:</strong> Points closer to prediction get higher weight</li>
        <li><strong>Gaussian kernel:</strong> Weight function based on distance</li>
        <li><strong>Bandwidth parameter:</strong> Controls locality of fit</li>
      </ul>

      <div class="formula">
        <strong>Objective:</strong> min Σ (yᵢ - β₀ - (x - xᵢ)ᵀβ₁)² Kₕ(x - xᵢ)<br>
        <strong>Solution:</strong> β̂ = (XᵀWX)⁻¹XᵀWY<br>
        <strong>Gaussian Kernel:</strong> Kₕ(z) = exp(-||z||²/(2h²))
      </div>

      <h4>Matrix Definitions</h4>
      <ul>
        <li><strong>X:</strong> (n × (p+1)) design matrix [1, (x-x₁)ᵀ; 1, (x-x₂)ᵀ; ...]</li>
        <li><strong>W:</strong> (n × n) diagonal weight matrix with Wᵢᵢ = Kₕ(x - xᵢ)</li>
        <li><strong>Y:</strong> (n × 1) response vector [y₁, y₂, ..., yₙ]ᵀ</li>
      </ul>

      <h3>Code Examples</h3>
      <div class="code-section">
        <div class="code-label">LWLR Implementation</div>
        <pre>def gaussian_kernel(z, h):
    """Gaussian kernel function"""
    return np.exp(-np.sum(z**2, axis=-1) / (2 * h**2))

def locally_weighted_regression(X_train, y_train, x_pred, h):
    """LWLR at single prediction point"""
    n, p = X_train.shape
    
    # Compute weights
    weights = gaussian_kernel(X_train - x_pred, h)
    
    # Design matrix: [1, (X_train - x_pred)]
    X_design = np.column_stack([np.ones(n), X_train - x_pred])
    
    # Weighted least squares
    W = np.diag(weights)
    XTW = X_design.T @ W
    beta = np.linalg.solve(XTW @ X_design, XTW @ y_train)
    
    # Prediction is β₀
    return beta[0]

# Cross-validation for bandwidth selection
def cross_validate_lwlr(X, y, h_values, cv_folds=5):
    kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)
    cv_errors = []
    
    for h in h_values:
        fold_errors = []
        for train_idx, val_idx in kf.split(X):
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            
            y_pred = [locally_weighted_regression(X_train, y_train, x, h) 
                     for x in X_val.flatten()]
            mse = np.mean((y_val - np.array(y_pred))**2)
            fold_errors.append(mse)
        
        cv_errors.append(np.mean(fold_errors))
    
    return np.array(cv_errors)</pre>
      </div>

      <h3>Best Practices</h3>
      <div class="tip-box">
        <h5>LWLR Implementation Tips</h5>
        <p>• <strong>Bandwidth selection:</strong> Use cross-validation for optimal h</p>
        <p>• <strong>Numerical stability:</strong> Add small regularization to XᵀWX</p>
        <p>• <strong>Computational efficiency:</strong> Pre-compute distances when possible</p>
        <p>• <strong>Kernel choice:</strong> Gaussian is most common, but others work</p>
      </div>

      <h3>Troubleshooting</h3>
      <div class="warning-box">
        <h5>Implementation Issues</h5>
        <p>• <strong>Singular matrix:</strong> Add regularization or increase bandwidth</p>
        <p>• <strong>Poor predictions:</strong> Check bandwidth selection and data quality</p>
        <p>• <strong>Slow computation:</strong> Use approximate methods for large datasets</p>
        <p>• <strong>Boundary effects:</strong> Consider different kernels or padding</p>
      </div>
    </div>
  </section>

  <!-- ======= 12. BIAS-VARIANCE TRADEOFF ======= -->
  <section class="section accent" id="bias-variance-tradeoff">
    <div class="wrap">
      <h2>⚖️ Bias-Variance Tradeoff</h2>
      <p class="muted">Understanding the fundamental tradeoff in machine learning</p>

      <h3>Installation & Setup</h3>
      <div class="code-section">
        <div class="code-label">Required packages</div>
        <pre>import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve, validation_curve</pre>
      </div>

      <h3>Core Concepts</h3>
      
      <h4>Fundamental Decomposition</h4>
      <div class="formula">
        <strong>Total Error = Bias² + Variance + Irreducible Error</strong><br>
        <strong>Bias:</strong> E[f̂(x)] - f(x)<br>
        <strong>Variance:</strong> E[(f̂(x) - E[f̂(x)])²]
      </div>

      <h4>Model Complexity Effects</h4>
      <ul>
        <li><strong>High Bias (Underfitting):</strong>
          <ul>
            <li>Model too simple to capture true relationship</li>
            <li>Poor performance on both training and test data</li>
            <li>Examples: Linear model for nonlinear data</li>
          </ul>
        </li>
        <li><strong>High Variance (Overfitting):</strong>
          <ul>
            <li>Model too complex, fits noise in training data</li>
            <li>Good training performance, poor test performance</li>
            <li>Examples: Deep decision tree, high-degree polynomial</li>
          </ul>
        </li>
      </ul>

      <h4>Method-Specific Tradeoffs</h4>
      <ul>
        <li><strong>LWLR:</strong> Bandwidth h controls tradeoff (small h = low bias, high variance)</li>
        <li><strong>Decision Trees:</strong> Depth controls tradeoff</li>
        <li><strong>Regularization:</strong> λ parameter controls tradeoff</li>
        <li><strong>Neural Networks:</strong> Architecture and training time</li>
      </ul>

      <h3>Code Examples</h3>
      <div class="code-section">
        <div class="code-label">Bias-Variance Analysis</div>
        <pre># Validation curve to visualize bias-variance tradeoff
from sklearn.model_selection import validation_curve

# Example: Decision tree depth
param_range = np.arange(1, 11)
train_scores, test_scores = validation_curve(
    DecisionTreeClassifier(random_state=42), 
    X, y, param_name='max_depth', param_range=param_range,
    cv=5, scoring='accuracy'
)

# Plot bias-variance tradeoff
plt.figure(figsize=(10, 6))
plt.plot(param_range, np.mean(train_scores, axis=1), 'o-', label='Training Score')
plt.plot(param_range, np.mean(test_scores, axis=1), 'o-', label='Validation Score')
plt.xlabel('Max Depth (Model Complexity)')
plt.ylabel('Accuracy')
plt.title('Bias-Variance Tradeoff')
plt.legend()
plt.grid(True)

# Bias-variance decomposition (conceptual)
def bias_variance_decomposition(true_function, model_predictions):
    """
    Conceptual bias-variance decomposition
    """
    mean_prediction = np.mean(model_predictions, axis=0)
    bias_squared = np.mean((mean_prediction - true_function)**2)
    variance = np.mean(np.var(model_predictions, axis=0))
    return bias_squared, variance</pre>
      </div>

      <h3>Best Practices</h3>
      <div class="tip-box">
        <h5>Managing Bias-Variance Tradeoff</h5>
        <p>• <strong>Cross-validation:</strong> Use for model selection and hyperparameter tuning</p>
        <p>• <strong>Regularization:</strong> Add penalty terms to control model complexity</p>
        <p>• <strong>Ensemble methods:</strong> Combine multiple models to reduce variance</p>
        <p>• <strong>Data size:</strong> More data generally reduces variance</p>
      </div>

      <h3>Troubleshooting</h3>
      <div class="warning-box">
        <h5>Diagnosis and Solutions</h5>
        <p>• <strong>High Bias:</strong> Increase model complexity, add features, reduce regularization</p>
        <p>• <strong>High Variance:</strong> Decrease complexity, add regularization, get more data</p>
        <p>• <strong>Both High:</strong> Review model choice and data quality</p>
        <p>• <strong>Validation:</strong> Use learning curves to diagnose the problem</p>
      </div>
    </div>
  </section>

  <!-- ======= 13. CROSS-VALIDATION TECHNIQUES ======= -->
  <section class="section dark" id="cross-validation">
    <div class="wrap">
      <h2>✅ Cross-Validation Techniques</h2>
      <p class="muted">Robust model evaluation and selection methods</p>

      <h3>Installation & Setup</h3>
      <div class="code-section">
        <div class="code-label">Required imports</div>
        <pre>from sklearn.model_selection import (
    cross_val_score, KFold, StratifiedKFold, 
    LeaveOneOut, cross_validate, GridSearchCV
)</pre>
      </div>

      <h3>Core Concepts</h3>
      
      <h4>Types of Cross-Validation</h4>
      <ul>
        <li><strong>K-Fold CV:</strong> Split data into k folds, train on k-1, test on 1</li>
        <li><strong>Stratified K-Fold:</strong> Maintains class distribution in each fold</li>
        <li><strong>Leave-One-Out (LOO):</strong> k = n, leave one sample out each time</li>
        <li><strong>Time Series CV:</strong> Respect temporal order in splits</li>
      </ul>

      <h4>Applications</h4>
      <ul>
        <li><strong>Model evaluation:</strong> Unbiased estimate of model performance</li>
        <li><strong>Hyperparameter tuning:</strong> Select optimal parameters</li>
        <li><strong>Model selection:</strong> Compare different algorithms</li>
        <li><strong>Feature selection:</strong> Evaluate feature importance</li>
      </ul>

      <div class="formula">
        <strong>K-Fold CV Error:</strong> CV = (1/k) Σ L(fᵢ, Dᵢ)<br>
        <strong>Standard Error:</strong> SE = √(Var(CV errors)/k)
      </div>

      <h3>Code Examples</h3>
      <div class="code-section">
        <div class="code-label">Cross-Validation Implementation</div>
        <pre># Basic cross-validation
from sklearn.model_selection import cross_val_score

# 5-fold cross-validation
scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
print(f"CV Score: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})")

# Stratified K-Fold for classification
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')

# Manual cross-validation loop
kf = KFold(n_splits=5, shuffle=True, random_state=42)
cv_errors = []

for train_idx, val_idx in kf.split(X):
    X_train, X_val = X[train_idx], X[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]
    
    model.fit(X_train, y_train)
    y_pred = model.predict(X_val)
    error = 1 - accuracy_score(y_val, y_pred)
    cv_errors.append(error)

cv_error = np.mean(cv_errors)
cv_std = np.std(cv_errors)

# Grid search with cross-validation
from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [3, 5, 7, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid, cv=5, scoring='accuracy', n_jobs=-1
)
grid_search.fit(X, y)

print(f"Best parameters: {grid_search.best_params_}")
print(f"Best CV score: {grid_search.best_score_:.3f}")</pre>
      </div>

      <h3>Best Practices</h3>
      <div class="tip-box">
        <h5>Cross-Validation Guidelines</h5>
        <p>• <strong>Fold selection:</strong> 5-10 folds typically optimal (bias-variance tradeoff)</p>
        <p>• <strong>Stratification:</strong> Use for classification to maintain class balance</p>
        <p>• <strong>Shuffle data:</strong> Unless temporal dependencies exist</p>
        <p>• <strong>Nested CV:</strong> Use for unbiased model comparison with hyperparameter tuning</p>
      </div>

      <h3>Troubleshooting</h3>
      <div class="warning-box">
        <h5>Common Pitfalls</h5>
        <p>• <strong>Data leakage:</strong> Ensure no information flows from test to train</p>
        <p>• <strong>Small datasets:</strong> Use LOO or stratified sampling</p>
        <p>• <strong>Imbalanced data:</strong> Use stratified CV and appropriate metrics</p>
        <p>• <strong>Time series:</strong> Use time-based splits, not random</p>
      </div>
    </div>
  </section>

  <!-- ======= 14. TROUBLESHOOTING GUIDE ======= -->
  <section class="section" id="troubleshooting">
    <div class="wrap">
      <h2>🔧 Common Issues & Solutions</h2>
      <p class="muted">Solutions to frequent problems in data processing and machine learning</p>

      <h3>OpenRefine Issues</h3>

      <div class="warning-box">
        <h5>⚠️ "GREL Expression Error"</h5>
        <p><strong>Problem:</strong> GREL syntax errors or unexpected results</p>
        <p><strong>Solutions:</strong></p>
        <p>• Test expressions on small data samples first</p>
        <p>• Use parentheses to ensure proper order of operations</p>
        <p>• Check for typos in function names (case-sensitive)</p>
        <p>• Use <code>value.toString()</code> if working with numbers</p>
      </div>

      <div class="warning-box">
        <h5>⚠️ "Row Count Mismatch After Filtering"</h5>
        <p><strong>Problem:</strong> Lost rows during data processing</p>
        <p><strong>Solutions:</strong></p>
        <p>• Check for case sensitivity in filters</p>
        <p>• Verify GREL expressions don't return empty strings</p>
        <p>• Use facets to inspect data before filtering</p>
        <p>• Export history to track which operation caused loss</p>
      </div>

      <h3>Python/Pandas Issues</h3>

      <div class="warning-box">
        <h5>⚠️ "KeyError: Column not found"</h5>
        <p><strong>Problem:</strong> Referencing non-existent column</p>
        <div class="code-section">
          <div class="code-label">Solution</div>
          <pre># Check available columns
print(df.columns.tolist())

# Safe column access
if 'column_name' in df.columns:
    result = df['column_name']
else:
    print("Column not found")

# Use .get() method for safe access
value = df.get('column_name', default_value)</pre>
        </div>
      </div>

      <div class="warning-box">
        <h5>⚠️ "SettingWithCopyWarning"</h5>
        <p><strong>Problem:</strong> Pandas warning about chained assignments</p>
        <div class="code-section">
          <div class="code-label">Solution</div>
          <pre># Avoid chained assignment
# Bad: df[df['A'] > 5]['B'] = 'new_value'

# Good: Use .loc
df.loc[df['A'] > 5, 'B'] = 'new_value'

# Or create explicit copy
subset = df[df['A'] > 5].copy()
subset['B'] = 'new_value'</pre>
        </div>
      </div>

      <h3>Machine Learning Issues</h3>

      <div class="warning-box">
        <h5>⚠️ "Model Overfitting"</h5>
        <p><strong>Problem:</strong> High training accuracy, poor test performance</p>
        <p><strong>Solutions:</strong></p>
        <p>• Reduce model complexity (less depth, fewer features)</p>
        <p>• Add regularization (L1/L2 penalties)</p>
        <p>• Use cross-validation for hyperparameter tuning</p>
        <p>• Collect more training data</p>
        <p>• Use ensemble methods to reduce variance</p>
      </div>

      <div class="warning-box">
        <h5>⚠️ "Poor Cross-Validation Results"</h5>
        <p><strong>Problem:</strong> High variance in CV scores</p>
        <p><strong>Solutions:</strong></p>
        <p>• Check data distribution and class balance</p>
        <p>• Use stratified CV for classification</p>
        <p>• Increase number of CV folds</p>
        <p>• Check for data leakage</p>
        <p>• Ensure proper preprocessing</p>
      </div>

      <h3>Flask Issues</h3>

      <div class="warning-box">
        <h5>⚠️ "Template Not Found"</h5>
        <p><strong>Problem:</strong> Flask can't locate HTML templates</p>
        <p><strong>Solutions:</strong></p>
        <p>• Ensure templates are in <code>templates/</code> folder</p>
        <p>• Check file name spelling and case</p>
        <p>• Verify Flask app initialization includes template folder</p>
        <div class="code-section">
          <div class="code-label">Correct Structure</div>
          <pre>app = Flask(__name__, template_folder='templates')</pre>
        </div>
      </div>

      <h3>RegEx Debugging</h3>

      <div class="tip-box">
        <h5>💡 RegEx Testing Strategy</h5>
        <p>• Use online tools: regex101.com, regexr.com</p>
        <p>• Test patterns incrementally (build complexity gradually)</p>
        <p>• Print intermediate results to debug extraction</p>
        <div class="code-section">
          <div class="code-label">Debug RegEx Pattern</div>
          <pre>import re

def debug_regex(pattern, text):
    """Debug regex pattern with detailed output"""
    print(f"Pattern: {pattern}")
    print(f"Text: {text}")
    
    match = re.search(pattern, text)
    if match:
        print(f"Full match: '{match.group()}'")
        for i, group in enumerate(match.groups(), 1):
            print(f"Group {i}: '{group}'")
    else:
        print("No match found")
    
    # Test with findall for multiple matches
    all_matches = re.findall(pattern, text)
    print(f"All matches: {all_matches}")

# Usage
debug_regex(r'([A-Z][a-z]+)\s+(Airport)', 
           "Beijing Capital Airport and Shanghai Pudong Airport")</pre>
        </div>
      </div>
    </div>
  </section>

  <!-- ======= 15. FUTURE LEARNING AREAS ======= -->
  <section class="section dark" id="future">
    <div class="wrap">
      <h2>🚀 Future Learning Areas</h2>
      <p class="muted">Structured areas for expanding knowledge and skills</p>

      <h3>Advanced Machine Learning</h3>
      <div class="future-section">
        <h4>🧠 Deep Learning</h4>
        <p>Add notes on: neural networks, TensorFlow/PyTorch, CNNs, RNNs, transformers, deep learning architectures</p>
      </div>

      <div class="future-section">
        <h4>📊 Advanced ML Algorithms</h4>
        <p>Add tutorials for: SVM kernels, clustering algorithms, dimensionality reduction, reinforcement learning</p>
      </div>

      <h3>Data Science & Analytics</h3>
      <div class="future-section">
        <h4>📈 Data Visualization</h4>
        <p>Add tutorials for: matplotlib, seaborn, plotly, dashboard creation, interactive charts</p>
      </div>

      <div class="future-section">
        <h4>🔍 Feature Engineering</h4>
        <p>Add content on: feature selection, feature creation, text processing, time series features</p>
      </div>

      <h3>Web Development</h3>
      <div class="future-section">
        <h4>🌐 Advanced Flask</h4>
        <p>Add content on: database integration, user authentication, API development, deployment strategies</p>
      </div>

      <div class="future-section">
        <h4>⚛️ Frontend Technologies</h4>
        <p>Add learning notes for: JavaScript, React, HTML/CSS advanced techniques, responsive design</p>
      </div>

      <h3>Database & SQL</h3>
      <div class="future-section">
        <h4>🗄️ Database Design</h4>
        <p>Add tutorials on: SQL queries, database normalization, performance optimization, NoSQL databases</p>
      </div>

      <h3>Tools & Automation</h3>
      <div class="future-section">
        <h4>🔧 Development Tools</h4>
        <p>Add guides for: Git version control, Docker containerization, CI/CD pipelines, testing frameworks</p>
      </div>

      <div class="future-section">
        <h4>☁️ Cloud Platforms</h4>
        <p>Add learning for: AWS, Azure, Google Cloud, serverless computing, cloud databases</p>
      </div>

      <h3>How to Add New Learning Sections</h3>
      <div class="tip-box">
        <h5>📝 Template for New Topics</h5>
        <p>1. Create a new section with appropriate ID</p>
        <p>2. Include: Overview, Installation/Setup, Core Concepts, Code Examples, Best Practices</p>
        <p>3. Add practical examples and real-world use cases</p>
        <p>4. Include troubleshooting section for common issues</p>
        <p>5. Update table of contents with new section</p>
      </div>
    </div>
  </section>

  <!-- ======= FOOTER ======= -->
  <footer>
    © <span id="year"></span> Tanvir Bakther • Comprehensive Learning Reference
  </footer>

  <script>
    // Navbar scroll effect
    const navbar = document.getElementById('navbar');
    addEventListener('scroll', () => {
      if (scrollY > 80) navbar.classList.add('scrolled'); 
      else navbar.classList.remove('scrolled');
    });

    // Set current year
    document.getElementById('year').textContent = new Date().getFullYear();

    // Smooth scrolling for anchor links
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const target = document.querySelector(this.getAttribute('href'));
        if (target) {
          target.scrollIntoView({ behavior: 'smooth', block: 'start' });
        }
      });
    });

    // Copy code functionality (optional enhancement)
    document.querySelectorAll('.code-section').forEach(section => {
      const pre = section.querySelector('pre');
      if (pre) {
        section.style.position = 'relative';
        const copyBtn = document.createElement('button');
        copyBtn.textContent = 'Copy';
        copyBtn.style.cssText = `
          position: absolute;
          top: 8px;
          right: 8px;
          background: #d4af37;
          color: #000;
          border: none;
          padding: 4px 8px;
          border-radius: 4px;
          font-size: 12px;
          cursor: pointer;
        `;
        
        copyBtn.addEventListener('click', () => {
          navigator.clipboard.writeText(pre.textContent).then(() => {
            copyBtn.textContent = 'Copied!';
            setTimeout(() => copyBtn.textContent = 'Copy', 2000);
          });
        });
        
        section.appendChild(copyBtn);
      }
    });
  </script>
</body>
</html>
